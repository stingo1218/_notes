1、数据产生方式经历的几个阶段。

> （1）运营式系统阶段
>
> （2）用户原创内容阶段
>
> （3）感知式系统阶段

2、试述大数据的4个基本特征。

> ①数据量大
>
> ②数据类型繁多
>
> ③<u>处理速度快</u>
>
> ④价值密度低。

3、试述大数据对思维方式的影响。

> 全样而非抽样,效率而非精确,相关而非因果。

4、试述Hadoop生态系统及每个部分的具体功能。

> Hadoop 的功能是利用服务器集群，根据用户自定义业务逻辑对海量数据进行分布式处理。
>
> - HDFS：分布式文件系统；
> - MapReduce：<u>分布式并行编程模型</u>；
> - YARN：<u>资源管理和调度器</u>；
> - Tez：运行在<u>YARN之下的下一代Hadoop查询处理框架</u>；
> - Hive：<u>Hadoop上的数据仓库</u>；
> - HBase：<u>Hadoop上的非关系型的分布式数据库</u>；
> - Pig：一个基于<u>Hadoop的大规模数据分析平台</u>，提供类似SQL的<u>查询语言Pig Latin</u>；
> - Sqoop：用于在<u>Hadoop与传统数据库之间进行数据传递</u>；
> - Oozie：Hadoop上的<u>工作流程管理系统</u>；
> - ZooKeeper：提供<u>分布式协调一致性服务</u>；
> - Storm：<u>流计算框架</u>；
> - Flume：一个高可用的，高可靠的，<u>分布式的海量日志采集、聚合和传输的系统</u>；
> - Ambari：<u>Hadoop快速部署工具</u>，支持Apache Hadoop集群的<u>供应、管理和监控</u>；
> - Kafka：一种高吞吐量的<u>分布式发布订阅消息系统</u>，可以<u>处理消费者规模的网站中的所有动作流数据</u>；
> - Spark：类似于<u>Hadoop MapReduce的通用并行框架</u>。

1、试述HDFS中名称节点和数据节点的具体功能。

> 答：
>
> **名称节点**（NameNode）
>
> - 负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构，即FsImage和EditLog。记录了每个文件中各个块所在的数据节点的位置信息。
>
> **数据节点**（DateNode）
>
> - 是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表。

2、请阐述HDFS在不发生故障的情况下读文件和写文件的过程。

> 答：
>
> **读数据过程**：
>
> (1)客户端通过FileSystem.open打开文件，相应地，在HDFS文件系统中DistributedFileSystem具体实现了FileSystem。因此，调用open()方法后，DistributedFileSystem会创建输入流 FSDataInputStream，对于HDFS而言，具体的输入流就是DFSInputStream。
> (2)在DFSInputStream的构造函数中，输入流通过ClientProtocal.getBlockLocations()远程调用名称节点，获得文件开始部分数据块的保存位置。对于该数据块，名称节点返回保存该数据块的所有数据节点的地址，同时根据距离客户端的远近对数据节点进行排序;然后 DistributedFileSystem会利用DFSInputStream来实例化FSDataInputStream，返回给客户端，同时返回了数据块的数据节点地址。
> (3)获得输入流FSDataInputStream后，客户端调用read函数开始读取数据。输入流根据前面的排序结果，选择距离客户端最近的数据节点建立连接并读取数据。
> (4)数据从该数据节点读到客户端;当该数据块读取完毕时，FSDataInputStream关闭和该数据节点的连接。
> (5)输入流通过getBlockLocations()方法查找下一个数据块(如果客户端缓存中已经包含了该数据块的位置信息，就不需要调用该方法)。
> (6)找到该数据块的最佳数据节点，读取数据。
> (7)当客户端读取完毕数据的时候，调用FSDataInputStream的close()函数，关闭输入流。需要注意的是，在读取数据的过程中，如果客户端与数据节点通信时出现错误，就会尝试连接包含此数据块的下一个数据节点。

> **写数据过程**：
>
> (1)客户端先通过FileSystem.create()方法创建一个文件，相应地，在HDFS 中Distributed FileSystem 具体实现了FileSystem。调用create()方法,DistributedFileSystem 会创建输出流 FSDataOutputStream，对于HDFS而言，具体的输出流就是DFSOutputStream。
> (2)然后，FSData OutputStream输出流 通过 RPC远程调用名称节点在文件系统的命名空间中创建一个新的文件。新建文件之前名称节点会先执行一些检查。检查过之后，名称节点会创造一个新文件，添加文件信息。并通过这个输出流返回来。
> (3)获得输出流 FSDataOutputStream 以后，客户端调用输出流的write()方法向HDFS 中对应的文件写入数据。
> (4)采用“流水线复制”的方法写入数据。客户端向输出流中写入的数据会首先被分成一个个的分包，这些分包被放人DFSOutputStream对象的内部队列。输出流DFSOutputStream会向名称节点申请保存文件和副本数据块的若干个数据节点，然后，这些数据节点形成一个数据流管道。队列中的分包最后被打包成数据包，发往数据流管道中的第1个数据节点写好以后，第1个数据节点将数据包发送给第2个数据节点写入，以此类推, 数据包会流经管道上的各个数据节点。
> (5)各个数据节点位于不同的机器上，数据需要通过网络发送。为了保证所有数据节点的数据都是准确的，接收到数据的数据节点要向发送者发送“确认包”(ACK Packet)。确认包沿着数据流管道“逆流而上”，从数据流管道依次经过各个数据节点并最终发往客户端，当客户端收到应答时，它将对应的分包从内部队列移除。不断执行(3)~(5)步，直到数据全部写完。
> (6)客户端调用close()方法关闭输出流，客户端不会再向输出流中写入数据，当DFSOutputStream 对象内部队列中的分包都收到应答以后，就可以使用ClientProtocol.complete()方法通知名称节点关闭文件，完成一次正常的写文件过程。

3、请阐述HBase和传统的关系数据库的区别。

> 1)**数据类型**：
>
> 关系数据库采用关系模型，具有丰富的数据类型和存储方式，HBase则采用了更加简单的数据模型，它把数据存储为未经解释的字符串。 
>
> 2)**数据操作**：
>
> 关系数据库中包含了丰富的操作，其中会涉及复杂的多表连接。HBase操作则不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空等，因为HBase在设计上就避免了复杂的表和表之间的关系。
>
> 3)**存储模式**：
>
> 关系数据库是基于行模式存储的。HBase是基于列存储的，每个列族都由几个文件保存，不同列族的文件是分离的。 
>
> 4)**数据索引**：
>
> 关系数据库通常可以针对不同列构建复杂的多个索引，以提高数据访问性能。HBase只有一个索引——行键，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来。 
>
> 5)**数据维护**：
>
> 在关系数据库中，更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后就不会存在。而在HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留。 
>
> 6)**可伸缩性**：
>
> 关系数据库很难实现横向扩展，纵向扩展的空间也比较有限。

5、试述HBase各功能组件及其作用。

> HBase的实现包括三个主要的功能组件：
>
> （1）库函数：链接到每个客户端
>
> （2）一个Master主服务器
>
> （3）许多个Region服务器
>
> 主服务器Master负责管理和维护HBase表的分区信息，维护Region服务器列表，分配Region，负载均衡。
> Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求。

6、请阐述在HBase三层结构下，客户端是如何访问到数据的。

> 首先访问Zookeeper，获取-ROOT表的位置信息，然后访问-Root-表，获得.MATA.表的信息，接着访问.MATA.表，找到所需的Region具体位于哪个Region服务器，最后才会到该Region服务器读取数据。

1、试述NoSQL数据的四大类型及每种类型的代表性产品？

> （1）键值数据库
> 相关产品：Redis、Memcached
> （2）列族数据库
> 相关产品：BigTable、HBase、Cassandra
> （3）文档数据库
> 相关产品：MongoDB、CouchDB、
> （4）图形数据库
> 相关产品：Neo4J、Infinte Graph

2、NoSQL数据库的三大理论基石是什么？

> CAP理论、BASE理论、最终一致性

3、试述CAP理论的具体含义？

> CAP理论，指的是在一个分布式系统中，Consistency(一致性)、Availability(可用性)、Partition Tolerance(分区容错性)，不能同时成立。
> C(Consistency):一致性,是指任何一个读操作总是能够读到之前完成的写操作的结果,也就是在分布式环境中,多点的数据是一致的,或者说,所有节点在同一时间具有相同的数据。 
> A:(Availability):可用性,是指快速获取数据,可以在确定的时间内返回操作结果,保证每个请求不管成功或者失败都有响应; 
> P(Tolerance of Network Partition):分区容忍性,是指当出现网络分区的情况时(即系统中的一部分节点无法和其他节点进行通信),分离的系统也能够正常运行,也就是说,系统中任意信息的丢失或失败不会影响系统的继续运作。

4、试述BASE理论的具体含义？

> BASE理论是指，Basically Available（基本可用）、Soft-state（ 软状态）、Eventual Consistency（最终一致性）。是基于CAP定理演化而来，是对CAP中一致性和可用性权衡的结果。核心思想：即使无法做到强一致性，但每个业务根据自身的特点，采用适当的方式来使系统达到最终一致性。

5、云计算的三种服务模式？

> IaaS（Infrastructure as a Service），基础设施即服务。指把IT基础设施作为一种服务通过网络对外提供，并根据用户对资源的实际使用量或占用量进行计费的一种服务模式。
> PaaS（Platform as a Service）平台即服务。 把服务器平台作为一种服务提供的商业模式，它包括操作系统和围绕特定应用的必需的服务。
> SaaS（Software-as-a-Service），软件即服务。即通过网络提供软件服务。

6、UMP系统怎样实现容灾？

> UMP系统会为每个用户创建两个MySQL实例，一个是主库，一个是从库，互相把对方设置为备份机。Zookeeper实时监听各个MySOL实例的状态，一旦发现主库宕机，立刻通知给Controller服务器。 Controller服务器启动主从切换操作，在路由表中修改用户名到后端MySOL实例地址的映射关系，把主库标记为不可用，借助于消息队列中间件RabbitMQ通知所有Proxy服务器修改用户名到后端MySOL实例地址的映射关系。然后主从切换完成，用户名被赋予一个新的可以正常使用的MvSOL实例，而这一切对于用户自己而言是完全透明的。
> 宕机后的主库在进行恢复处理后需要再次上线。在主库恢复时，会把从库中的更新都复制给自己，当主库的数据库状态快要达到和从库一致的状态时，Controller服务器就会命令从库停止更新，进入不可写状态，禁止用户写入数据。等到主库更新到和从库完全一致的状态时，Controller服务器就会发起主从切换操作，并在路由表中把主库标记为可用状态，然后通知Proxy服务器把写操作切回主库上，用户写操作可以继续执行，之后再把从库修改为可写状态。



7、UMP系统采用那两种方式实现资源隔离？(不是大题)

> UMP有两种资源隔离方式：
> 一种是用Cgroup限制MySQL进程资源，它适用于多个MySQL实例共享一台物理机的情况；
> 一种是在Proxy服务器端限制QPS，它适用于多个用户共享同一个MySQL实例的情况。

1.MapReduce模型采用Master(JobTracker)-Slave(TaskTracker)结构，试描述JobTracker和 TaskTracker的功能。

> 答：JobTracker：
> JobTracker负责资源监控和作业调度。JobTracker监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点。JobTracker会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源。
> TaskTracker：
> 周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。使用“slot”等量划分本节点上的资源量，Hadoop调度器将各个TaskTracker上的空闲slot分配给Task使用，slot分为Map slot和Reduce slot两种，分别供MapTask和Reduce Task使用。

2.MapReduce计算模型的核心是Map函数和Reduce函数，试述这两个函数各自的输入、输出以及处理过程。(选择题)
函数	输入	输出
Map	<k1,v1>	List(<k2,v2>)
Reduce	<k2,List(v2)>	<k3,v3>

> 答：Map函数的输入是来自于分布式文件系统的文件块，这些文件块的格式是任意的，可以是文档，也可以是二进制格式。文件块是一系列元素的集合，这些元素是任意类型的，同一个元素不能跨文件块存储。Map函数将输入的元素转换成<key，value>形式的键值对，键和值的类型也是任意的，其中，键不同于一般的标志属性，即键没有唯一性，不能作为输出的身份标识，即使是同一输入元素，也可通过一个Map任务生成具有相同键的多个<key，value>。
> Reduce 函数的任务就是将输入的一系列具有相同键的键值对以某种方式组合起来，输出处理后的键值对，输出结果会合并成一个文件。用户可以指定Reduce任务的个数（如n个），并通知实现系统，然后主控进程通常会选择一个Hash函数，Map任务输出的每个键都会经过Hash函数计算，并根据哈希结果将该键值对输入相应的Reduce任务来处理。对于处理键为k的Reduce任务的输入形式为<K，<V1，v2.……vn>>，输出为<k，V>。

3.分别描述Map端和Reduce端的Shuffle过程(需包括Spill  Sort  Merge  Fetch的过程)。溢写、排序、归并、取数据

> 1）Map端的Shuffle过程：
> ①输入数据和执行Map任务：
> 输入<key，value>，按一定的映射规则转换成一批<key value>输出。
> ②写入缓存
> 每个Map任务都会被分配一个缓存（默认100MB），Map的输出结果首先写入缓存, 再写入磁盘。
> ③溢写（分区，排序和合并）
> 随着Map任务的执行，缓存满了以后要启动溢写(Spill)操作，在溢写到磁盘之前，缓存中的数据首先会被分区(Partition)。每个分区内的所有键值对，后台线程会根据key对它们进行内存排序(Sort)。排序结束后，如果用户定义了合并(Combine)操作，还需要对排序后的数据进行合并(Combine)操作。经过分区、排序以及合并操作之后，这些缓存中的键值对就被写入磁盘，并清空缓存。
> ④文件归并
> 每次溢写操作都会在磁盘中生成一个新的溢写文件，随着MapReduce任务的进行，磁盘中的溢写文件数量会越来越多。如果存在多个溢写文件，在Map任务全部结束之前，系统会对所有溢写文件中的数据进行归并(Merge)。进行文件归并时，如果磁盘中已经生成的溢写文件的数量超过参数minnumspillsfor.combine的值时(默认值是3，可以修改)，可以再次进行合并操作。
> 最后Map端的Shuffle过程全部完成，最终生成的一个大文件存放在本地磁盘。
>
> 2）Reduce端的Shuffle过程：
> ①“领取”数据：Map端的Shuffle过程结束后，所有Map输出结果都保存在Map机器的本地磁盘上。Reduce任务通过RPC向JobTracker询问Map任务是否已经完成，若完成，则领取数据放到自己所在机器上。
> ②归并数据：从Map端领回的数据会首先被存放在Reduce任务所在机器的缓存中，如果缓存被占满，就会被溢写到磁盘中。溢写过程启动时，具有相同key的键值对会被归并(Merge)，如果定义了合并操作可以再次进行合并，然后写入磁盘。
> ③把数据输入Reduce任务：磁盘中经过多轮归并后得到的若干个大文件直接输入给Reduce任务，这样可以减少磁盘读写开销。
> 到此整个Shuffe过程顺利结束。