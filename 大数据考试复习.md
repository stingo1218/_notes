### 1.1.3数据产生方式的变革促成大数据时代的来临

![image-20220502184116554](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502184123.png)

## 1.2大数据特性

![image-20220502184201085](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502184201.png)

**4V特性：数据量大 、数据类型多、处理速度快、价值密度低**

## 1.6大数据计算模式

![image-20220502184223453](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502184223.png)

### 1.8.1云计算

#### 1. 云计算概念

云计算实现了通过网络提供可伸缩的、廉价的分布式计算能力，用户只需要在具备网络接入条件的地方，就可以随时随地获得所需的各种IT资源

![image-20220502184243372](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502184243.png)

![image-20220502184250617](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502184250.png)

#### 3. 云计算数据中心

云计算数据中心是一整套复杂的设施，包括刀片服务器、宽带网络连接、环境控制设备、监控设备以及各种安全装置等

数据中心是云计算的重要载体，为云计算提供计算、存储、带宽等各种硬件资源，为各种平台和应用提供运行支撑环境

全国各地推进数据中心建设

# 第2章 大数据处理架构Hadoop

### 2.2.1 Hadoop简介

 Hadoop是Apache软件基金会旗下的一个开源分布式计算平台，为用户提供了系统底层细节透明的分布式基础架构。

 Hadoop是基于**Java****语言**开发的，具有很好的跨平台特性，并且可以部署在廉价的计算机集群中。

 Hadoop的**核心**是**分布式文件系统HDFS**（Hadoop Distributed File System）和**MapReduce**。

 Hadoop被公认为行业大数据标准开源软件，在分布式环境下提供了海量数据的处理能力。

 几乎所有主流厂商都围绕Hadoop提供开发工具、开源软件、商业化工具和技术服务，如谷歌、雅虎、微软、思科、淘宝等，都支持Hadoop。

### 2.2.3 Hadoop的特性

​    Hadoop是一个能够对大量数据进行分布式处理的软件框架，并且是以一种可靠、高效、可伸缩的方式进行处理的，它具有以下几个方面的特性：

-   **高可靠性**
-   **高效性**
-   **高可扩展性**
-   **高容错性**
-   **成本低**
- **运行在Linux平台上**
-   **支持多种编程语言**



## 2.3 Hadoop项目结构



Hadoop的项目结构不断丰富发展，已经形成一个丰富的Hadoop生态系统

![image-20220502184421322](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502184421.png)

![image-20220502184449352](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502184449.png)



#### 伪分布式安装配置

-  Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件
-  Hadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/ 中，伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml 
-  Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现

**修改配置文件** **core-site.xml** 

```xml
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

- hadoop.tmp.dir表示存放临时数据的目录，即包括NameNode的数据，也包括DataNode的数据。该路径任意指定，只要实际存在该文件夹即可 
- name为fs.defaultFS的值，表示hdfs路径的逻辑名称

**修改配置文件 hdfs-site.xml**

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
       <value>file:/usr/local/hadoop/tmp/dfs/data</value>
    </property></configuration>
```

- dfs.replication表示副本的数量，伪分布式要设置为1
- dfs.namenode.name.dir表示本地磁盘目录，是存储fsimage文件的地方
- dfs.datanode.data.dir表示本地磁盘目录，HDFS数据存放block的地方

# 第3章 分布式文件系统HDFS

### 3.1.2	分布式文件系统的结构

分布式文件系统在物理结构上是由计算机集群中的多个节点构成的，这些节点分为两类，一类叫“主节点”(Master Node)或者也被称为“名称结点”(NameNode)，另一类叫“从节点”（Slave Node）或者也被称为“数据节点”(DataNode)

![image-20220502184624736](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502184624.png)

## 3.2	HDFS简介

HDFS特殊的设计，在实现上述优良特性的同时，也使得自身具有一些应用局限性，主要包括以下几个方面：

-  不适合低延迟数据访问
- 无法高效存储大量小文件
- 不支持多用户写入及任意修改文件

### 3.3.1	块

**HDFS默认一个块64MB，一个文件被分成多个块，以块作为存储单位**

**块的大小远远大于普通文件系统，可以最小化寻址开销**

HDFS采用抽象的块概念可以带来以下几个明显的好处：

​    **●** 支持大规模文件存储：文件以块为单位进行存储，一个大规模文件可以被分拆成若干个文件块，不同的文件块可以被分发到不同的节点上，因此，一个文件的大小不会受到单个节点的存储容量的限制，可以远远大于网络中任意节点的存储容量

​    **●**  简化系统设计：首先，大大简化了存储管理，因为文件块大小是固定的，这样就可以很容易计算出一个节点可以存储多少文件块；其次，方便了元数据的管理，元数据不需要和文件块一起存储，可以由其他系统负责管理元数据

​    **●**  适合数据备份：每个文件块都可以冗余存储到多个节点上，大大提高了系统的容错性和可用性

### 3.3.2	名称节点和数据节点

![image-20220502185622220](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502185622.png)

**名称节点的数据结构**

- 在HDFS中，名称节点（NameNode）负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构，即**FsImage**和**EditLog**。
- **FsImage**用于维护文件系统树以及文件树中所有的文件和文件夹的元数据操作。**日志文件**EditLog中记录了所有针对文件的创建、删除、重命名等操作。
- 名称节点记录了每个文件中各个块所在的数据节点的位置信息。

![image-20220502210513544](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502210513.png)

**名称节点的启动**(了解)

- 在名称节点启动的时候，它会将FsImage文件中的内容加载到内存中，之后再执行EditLog文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作。
- 一旦在内存中成功建立文件系统元数据的映射，则创建一个新的FsImage文件和一个空的EditLog文件
- 名称节点起来之后，HDFS中的更新操作会重新写到EditLog文件中，因为FsImage文件一般都很大（GB级别的很常见），如果所有的更新操作都往FsImage文件中添加，这样会导致系统运行的十分缓慢，但是，如果往EditLog文件里面写就不会这样，因为EditLog 要小很多。每次执行写操作之后，且在向客户端发送成功代码之前，edits文件都需要同步更新

**名称节点运行期间EditLog不断变大的问题**

-   在名称节点运行期间，HDFS的所有更新操作都是直接写到EditLog中，久而久之， EditLog文件将会变得很大。
-   虽然这对名称节点运行时候是没有什么明显影响的，但是，当名称节点重启的时候，名称节点需要先将FsImage里面的所有内容映像到内存中，然后再一条一条地执行EditLog中的记录，当EditLog文件非常大的时候，会导致名称节点启动操作非常慢，而在这段时间内HDFS系统处于安全模式，一直无法对外提供写操作，影响了用户的使用。

**如何解决？    SecondaryNameNode第二名称节点**(重点)

- **第二名称节点**是HDFS架构中的一个组成部分，它是用来保存名称节点中对HDFS 元数据信息的备份，并减少名称节点重启的时间。**SecondaryNameNode一般是单独运行在一台机器上。**

**SecondaryNameNode的工作情况：**

1. SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别；
2. SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文件，并下载到本地的相应目录下；
3. SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；这个过程就是EditLog和FsImage文件合并；
4. SecondaryNameNode执行完（3）操作之后，会通过post方式将新的FsImage文件发送到NameNode节点上
5. NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了

![image-20220502210801314](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502210801.png)

**数据节点（DataNode）**

- 数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表。

- 每个数据节点中的数据会被保存在各自节点的**本地Linux文件系统中**

## 3.4	HDFS体系结构

### 3.4.1	HDFS体系结构概述

​        HDFS采用了主从（Master/Slave）结构模型，一个HDFS集群包括一个名称节点（NameNode）和若干个数据节点（DataNode）（如图3-4所示）。名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问。集群中的数据节点一般是一个节点运行一个数据节点进程，负责处理文件系统客户端的读/写请求，在名称节点的统一调度下进行数据块的创建、删除和复制等操作。每个数据节点的数据实际上是保存在本地Linux文件系统中的。

![image-20220502210842706](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502210842.png)

### 3.4.3	通信协议

- •HDFS是一个部署在集群上的分布式文件系统，因此，很多数据需要通过网络进行传输
- •所有的HDFS通信协议都是构建在**TCP/IP协议**基础之上的。
- •客户端通过一个可配置的端口向**名称节点**主动发起**TCP连接**，并使用客户端协议与**名称节点**进行交互
- •**名称节点**和**数据节点**之间则使用**数据节点协议**进行交互
- •客户端与数据节点的交互是通过**RPC**（Remote Procedure Call）来实现的。在设计上，名称节点不会主动发起RPC，而是响应来自客户端和数据节点的RPC请求

### 3.5.1	冗余数据保存

​    作为一个分布式文件系统，为了保证系统的容错性和可用性，HDFS采用了多副本方式对数据进行冗余存储，通常一个数据块的多个副本会被分布到不同的数据节点上，如图3-5所示，数据块1被分别存放到数据节点A和C上，数据块2被存放在数据节点A和B上。这种多副本方式具有以下几个**优点**：

  （1）**加快数据传输速度**

  （2）**容易检查数据错误**

  （3）**保证数据可靠性**

![image-20220502210925991](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502210926.png)

### 3.5.2	数据存取策略

#### 1.数据存放

- •第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑 选一台磁盘不太满、CPU不太忙的节点
- •第二个副本：放置在与第一个副本不同的机架的节点上
- •第三个副本：与第一个副本相同机架的其他节点上
- •更多副本：随机节点

![image-20220502210948825](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502210948.png)

#### 2. 数据读取(就近读取)

HDFS提供了一个API可以确定一个数据节点所属的机架ID，客户端也可以调用API获取自己所属的机架ID。

当客户端读取数据时，从名称节点获得数据块不同副本的存放位置列表，列表中包含了副本所在的数据节点，可以调用API来确定客户端和这些数据节点所属的机架ID，当发现某个数据块副本对应的机架ID和客户端对应的机架ID相同时，就优先选择该副本读取数据，如果没有发现，就随机选择一个副本读取数据。

### 3.5.3	数据错误与恢复

​        HDFS具有较高的容错性，可以兼容廉价的硬件，它把硬件出错看作一种常态，而不是异常，并设计了相应的机制检测数据错误和进行自动恢复，主要包括以下几种情形：名称节点出错、数据节点出错和数据出错。

#### 1.名称节点出错

​     名称节点保存了所有的元数据信息，其中，最核心的两大数据结构是FsImage和Editlog，如果这两个文件发生损坏，那么整个HDFS实例将失效。因此，HDFS设置了备份机制，把这些核心文件同步复制到备份服务器SecondaryNameNode上。当名称节点出错时，就可以根据备份服务器SecondaryNameNode中的FsImage和Editlog数据进行恢复。

#### 2.数据节点出错

- 每个数据节点会定期向名称节点发送“心跳”信息，向名称节点报告自己的状态。
- 当数据节点发生故障，或者网络发生断网时，名称节点就无法收到来自一些数据节点的心跳信息，这时，这些数据节点就会被标记为“宕机”，节点上面的所有数据都会被标记为“不可读”，名称节点不会再给它们发送任何I/O请求。
- 这时，有可能出现一种情形，即由于一些数据节点的不可用，会导致一些数据块的副本数量小于冗余因子。
  名称节点会定期检查这种情况，一旦发现某个数据块的副本数量小于冗余因子，就会启动数据冗余复制，为它生成新的副本。
- HDFS和其它分布式文件系统的最大区别就是可以调整冗余数据的位置。

#### 3. 数据出错

- •网络传输和磁盘错误等因素，都会造成数据错误。
- •客户端在读取到数据后，会采用md5和sha1对数据块进行校验，以确定读取到正确的数据。
- •在文件被创建时，客户端就会对每一个文件块进行信息摘录，并把这些信息写入到同一个路径的隐藏文件里面。
- •当客户端读取文件的时候，会先读取该信息文件，然后，利用该信息文件对每个读取的数据块进行校验，如果校验出错，客户端就会请求到另外一个数据节点读取该文件块，并且向名称节点报告这个文件块有错误，名称节点会定期检查并且重新复制这个块。

## 3.6	HDFS数据读写过程

- **•FileSystem是一个通用文件系统的抽象基类**，可以被分布式文件系统继承，所有可能使用Hadoop文件系统的代码，都要使用这个类。
- •Hadoop为FileSystem这个抽象类提供了多种具体实现。
- **•DistributedFileSystem**就是FileSystem在HDFS文件系统中的具体实现。
- •FileSystem的**open()方法**返回的是一个输入流FSDataInputStream对象，在HDFS文件系统中，具体的输入流就是**DFSInputStream**；FileSystem中的create()方法返回的是一个输出流FSDataOutputStream对象，在HDFS文件系统中，**具体的输出流就是DFSOutputStream**。

```java
Configuration conf = new Configuration();   //定义环境配置变量                                       
FileSystem fs = FileSystem.get(conf);  //定义FileSystem的一个实例fs
FSDataInputStream in = fs.open(new Path(uri));
FSDataOutputStream out = fs.create(new Path(uri));
```

### 3.6.1	读数据的过程

![image-20220502214915787](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502214915.png)

### 3.6.2	写数据的过程

![image-20220502214929534](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502214929.png)

# 第4章 分布式数据库HBase

### 4.1.3	HBase与传统关系数据库的对比分析(小题)

•HBase与传统的关系数据库的区别主要体现在以下几个方面：

•（1）**数据类型**：关系数据库采用关系模型，具有丰富的数据类型和存储方式，HBase则采用了更加简单的数据模型，它把数据存储为未经解释的字符串。

•（2）**数据操作**：关系数据库中包含了丰富的操作，其中会涉及复杂的多表连接。HBase操作则不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空等，因为HBase在设计上就避免了复杂的表和表之间的关系。

•（3）**存储模式**：关系数据库是基于行模式存储的。HBase是基于列存储的，每个列族都由几个文件保存，不同列族的文件是分离的

•（4）**数据索引**：关系数据库通常可以针对不同列构建复杂的多个索引，以提高数据访问性能。HBase只有一个索引——行键，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来。

•（5）**数据维护**：在关系数据库中，更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后就不会存在。而在HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留。

•（6）**可伸缩性**：关系数据库很难实现横向扩展，纵向扩展的空间也比较有限。相反，HBase和BigTable这些分布式数据库就是为了实现灵活的水平扩展而开发的，能够轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩。

### 4.3.1 数据模型概述

•HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、列族、列限定符和时间戳。。

•每个值是一个未经解释的字符串，没有数据类型。

•用户在表中存储数据，每一行都有一个可排序的行键和任意多的列。

•表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多个列，同一个列族里面的数据存储在一起。

•列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以**字符串**形式存储，用户需要自行进行数据类型转换。

•HBase中执行更新操作时，**并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留**（这是和HDFS只允许追加不允许修改的特性相关的）。

### 4.3.2 数据模型相关概念

•**表**：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族

•**行**：每个HBase表都由若干行组成，每个行由行键（row key）来标识。

•**列族**：一个HBase表被分组成许多“列族”（Column Family）的集合，它是基本的访问控制单元。

•**列限定符**：列族里的数据通过列限定符（或列）来定位。

•**单元格**：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字节数组byte[]。

•**时间戳**：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引。

![image-20220502215027052](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502215027.png)

### 4.3.3 数据坐标

•HBase中需要根据**行键、列族、列限定符**和**时间戳**来确定一个单元格，因此，可以视为一个“四维坐标”，即[**行键, 列族, 列限定符, 时间戳**]

![image-20220502215045910](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502215045.png)

### 4.3.4 概念视图

![image-20220502215055586](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502215055.png)

### 4.3.5 物理视图

![image-20220502215104772](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502215104.png)

### 4.4.1 HBase功能组件

•HBase的实现包括三个主要的功能组件：

–（1）库函数：链接到每个客户端

–（2）一个Master主服务器

–（3）许多个Region服务器

•主服务器Master负责管理和维护HBase表的分区信息，维护Region服务器列表，分配Region，负载均衡

•Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求

•客户端并不是直接从Master主服务器上读取数据，而是在获得Region的存储位置信息后，直接从Region服务器上读取数据

•客户端并不依赖Master，而是通过Zookeeper来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计方式使得Master负载很小 

### 4.4.3 Region的定位

•元数据表，又名.META.表，存储了Region和Region服务器的映射关系

•当HBase表很大时， .META.表也会被分裂成多个Region

•根数据表，又名-ROOT-表，记录所有元数据的具体位置

•-ROOT-表只有唯一一个Region，名字是在程序中被写死的

•Zookeeper文件记录了-ROOT-表的位置

![image-20220502215130851](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502215130.png)

![image-20220502215136096](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502215136.png)

客户端访问数据时的“三级寻址”

•为了加速寻址，客户端会缓存位置信息，同时，需要解决缓存失效问题

•寻址过程客户端只需要询问Zookeeper服务器，不需要连接Master服务器

![image-20220502215152702](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502215152.png)

## 4.5 HBase运行机制

### 4.5.1 HBase系统架构

![image-20220502215207286](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502215207.png)

•1. 客户端

–客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程

•2. Zookeeper服务器

–Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题

![image-20220502215229329](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502215229.png)

**•3. Master**

**•主服务器Master主要负责表和Region的管理工作：**

**–管理用户对表的增加、删除、修改、查询等操作**

**–实现不同Region服务器之间的负载均衡**

**–在Region分裂或合并后，负责重新调整Region的分布**

**–对发生故障失效的Region服务器上的Region进行迁移**

•4. Region服务器

–Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求

### 4.5.2 Region服务器工作原理

![image-20220502215838556](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502215838.png)

#### 1. 用户读写数据过程 

•用户写入数据时，被分配到相应Region服务器去执行

•用户数据首先被写入到MemStore和Hlog中

•只有当操作写入Hlog之后，commit()调用才会将其返回给客户端

•当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找

#### 2. 缓存的刷新

•系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记。

•每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件。

•每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务。

#### 3. StoreFile的合并

•每次刷写都生成一个新的StoreFile，数量太多，影响查找速度

•调用Store.compact()把多个合并成一个

•合并操作比较耗费资源，只有数量达到一个阈值才启动合并

### 4.5.3	Store工作原理

•Store是Region服务器的核心

•多个StoreFile合并成一个

•单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region

![image-20220502220040782](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502220040.png)

### 4.5.4	HLog工作原理

•分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复。

•HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log）。

•用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘。

•Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master

•Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录

•系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器

•Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复

•共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志

# 第5章  NoSQL数据库

## 5.3  NoSQL与关系数据库的比较

![image-20220502220112709](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502220112.png)

![image-20220502220120372](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502220120.png)

![image-20220502220142152](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502220142.png)

## 5.4  NoSQL的四大类型

NoSQL数据库虽然数量众多，但是，归结起来，典型的NoSQL数据库通常包括键值数据库、列族数据库、文档数据库和图数据库

![image-20220502220322276](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502220322.png)

![image-20220502220325921](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502220325.png)

![image-20220502220330412](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502220330.png)

## 5.5 NoSQL的三大基石

所谓的CAP指的是：

- **C**（Consistency）：一致性，是指任何一个读操作总是能够读到之前完成的写操作的结果，也就是在分布式环境中，多点的数据是一致的，或者说，所有节点在同一时间具有相同的数据
- **A**:（Availability）：可用性，是指快速获取数据，可以在确定的时间内返回操作结果，保证每个请求不管成功或者失败都有响应；
- **P**（Tolerance of Network Partition）：分区容忍性，是指当出现网络分区的情况时（即系统中的一部分节点无法和其他节点进行通信），分离的系统也能够正常运行，也就是说，系统中任意信息的丢失或失败不会影响系统的继续运作。

![image-20220502220430459](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502220430.png)

### 5.5.1 CAP

当处理CAP的问题时，可以有几个明显的选择：

1.CA：也就是强调一致性（C）和可用性（A），放弃分区容忍性（P），最简单的做法是把所有与事务相关的内容都放到同一台机器上。很显然，这种做法会严重影响系统的可扩展性。传统的关系数据库（MySQL、SQL Server和PostgreSQL），都采用了这种设计原则，因此，扩展性都比较差

2.CP：也就是强调一致性（C）和分区容忍性（P），放弃可用性（A），当出现网络分区的情况时，受影响的服务需要等待数据一致，因此在等待期间就无法对外提供服务

3.AP：也就是强调可用性（A）和分区容忍性（P），放弃一致性（C），允许系统返回不一致的数据

![image-20220502220503084](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502220503.png)

### 5.5.2 BASE

BASE的基本含义是基本可用（Basically Availble）、软状态（Soft-state）和最终一致性（Eventual consistency）：

- **基本可用**
  - ​    基本可用，是指一个分布式系统的一部分发生问题变得不可用时，其他部分仍然可以正常使用，也就是允许分区失败的情形出现
- **软状态**
  - ​    “软状态（soft-state）”是与“硬状态（hard-state）”相对应的一种提法。数据库保存的数据是“硬状态”时，可以保证数据一致性，即保证数据一直是正确的。“软状态”是指状态可以有一段时间不同步，具有一定的滞后性
- **最终一致性**
  - ​    一致性的类型包括强一致性和弱一致性，二者的主要区别在于高并发的数据访问操作下，后续操作是否能够获取最新的数据。对于强一致性而言，当执行完一次更新操作后，后续的其他读操作就可以保证读到更新后的最新数据；反之，如果不能保证后续访问读到 的都是更新后的最新数据，那么就是弱一致性。而最终一致性只不过是弱一致性的一种特例，允许后续的访问操作可以暂时读不到更新后的数据，但是经过一段时间之后，必须最终读到更新后的数据。
  -   最常见的实现最终一致性的系统是DNS（域名系统）。一个域名更新操作根据配置的形式被分发出去，并结合有过期机制的缓存；最终所有的客户端可以看到最新的值。

### 5.5.3  最终一致性(选择题)

最终一致性根据更新数据后各进程访问到数据的时间和方式的不同，又可以区分为：

- 因果一致性：如果进程A通知进程B它已更新了一个数据项，那么进程B的后续访问将获得A写入的最新值。而与进程A无因果关系的进程C的访问，仍然遵守一般的最终一致性规则
- “读己之所写”一致性：可以视为因果一致性的一个特例。当进程A自己执行一个更新操作之后，它自己总是可以访问到更新过的值，绝不会看到旧值
- 单调读一致性：如果进程已经看到过数据对象的某个值，那么任何后续访问都不会返回在那个值之前的值

最终一致性根据更新数据后各进程访问到数据的时间和方式的不同，又可以区分为：

- 会话一致性：它把访问存储系统的进程放到会话（session）的上下文中，只要会话还存在，系统就保证“读己之所写”一致性。如果由于某些失败情形令会话终止，就要建立新的会话，而且系统保证不会延续到新的会话
- 单调写一致性：系统保证来自同一个进程的写操作顺序执行。系统必须保证这种程度的一致性，否则就非常难以编程了

![image-20220502222756737](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502222756.png)

# 第6章 云数据库

### 6.3.1	UMP系统概述(选择题)

UMP系统是低成本和高性能的MySQL云数据库方案。

总的来说，**UMP系统架构设计遵循了以下原则**：

- •保持单一的系统对外入口，并且为系统内部维护单一的资源池。
- •消除单点故障，保证服务的高可用性。
- •保证系统具有良好的可伸缩性，能够动态地增加、删减计算与存储节点。
- •保证分配给用户的资源也是弹性可伸缩的，资源之间相互隔离，确保应用和数据安全。

### 6.3.2	UMP系统架构

![image-20220505150428327](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220505150502.png)

![image-20220502222904953](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502222905.png)

![image-20220502222918379](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502222918.png)

![image-20220502222940609](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502222940.png)

![image-20220502222950551](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502222950.png)

![image-20220502223010141](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223010.png)

![image-20220502223034321](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223034.png)

![image-20220502223040677](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223040.png)

![image-20220502223047102](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223047.png)

### 6.3.3	UMP系统功能

UMP系统是构建在一个大的集群之上的，通过多个组件的协同作业，整个系统实现了对用户透明的各种功能：

- •**容灾**
- •读写分离
- •分库分表
- •资源管理
- •资源调度
- •**资源隔离**
- •数据安全

![image-20220502223115775](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223115.png)

![image-20220502223121492](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223121.png)

# 第7章  MapReduce

### 7.1.2	MapReduce模型简介

![image-20220502223141523](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223141.png)

### 7.1.3 Map和Reduce函数

![image-20220502223154529](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223154.png)

## 7.2 MapReduce的体系结构

![image-20220502223647161](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223647.png)

![image-20220502223657805](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223657.png)

![image-20220502223732324](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223732.png)

## 7.3	MapReduce工作流程

### 7.3.1	工作流程概述

![image-20220502223750450](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223750.png)

### 7.3.2	MapReduce各个执行阶段

![image-20220502223757730](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223757.png)

![image-20220502223808414](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223808.png)

![image-20220502223813838](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223813.png)

### 7.3.3	Shuffle过程详解

#### 1. Shuffle过程简介

![image-20220502223841986](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223842.png)

#### 2. Map端的Shuffle过程

![image-20220502223852237](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223852.png)

#### 3. Reduce端的Shuffle过程

![image-20220502223904888](https://cdn.jsdelivr.net/gh/stingo1218/pic/img/20220502223905.png)